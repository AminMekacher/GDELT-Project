{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import pycountry\n",
    "import seaborn as sns\n",
    "\n",
    "import collections\n",
    "import jellyfish\n",
    "\n",
    "import bs4 as bs  \n",
    "import urllib3\n",
    "import shutil\n",
    "import re  \n",
    "import nltk\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_graph_creation(G, dataframe):\n",
    "    import numpy as np\n",
    "    actor_tot_list = []\n",
    "    actor_buffer_list = [] # List used to check if an article is a perfect replica of the previous one\n",
    "    for actor_list, theme_list in zip(dataframe.V2ENHANCEDPERSONS.unique(), dataframe.V2ENHANCEDTHEMES.unique()):\n",
    "        actor_temp_list, offset_temp_list = [], []\n",
    "        \n",
    "        if not isinstance(actor_list, float):\n",
    "            max_offset_diff = maximum_offset_difference(actor_list, theme_list)\n",
    "            for actor in actor_list.split(';'):\n",
    "                [actor_temp, offset_temp] = actor.split(',')\n",
    "            \n",
    "                if offset_temp not in offset_temp_list:\n",
    "                    offset_temp_list.append(offset_temp)\n",
    "                    \n",
    "                    # Compute similarity between actor_temp and all actors in the tot_list\n",
    "                    if actor_tot_list:\n",
    "                        similarity_max = np.max([jellyfish.jaro_winkler(actor_temp, actor2) for \n",
    "                                                 actor2 in actor_tot_list])\n",
    "                        index_max = np.argmax([jellyfish.jaro_winkler(actor_temp, actor2) for \n",
    "                                                  actor2 in actor_tot_list])\n",
    "                        actor_max = actor_tot_list[index_max]\n",
    "                        \n",
    "                        nb_identical_names = len(set(actor_temp.split(' ')) & set(actor_max.split(' ')))\n",
    "                    \n",
    "                    else:\n",
    "                        similarity_max = 0\n",
    "                        nb_identical_names = 0\n",
    "                        \n",
    "                    # Condition to correct the name if there is a misdetected 'A'\n",
    "                    if actor_temp[0:2] == 'A ':\n",
    "                        actor_temp = actor_temp[2:]   \n",
    "                    \n",
    "                    if 'Kanzler Joseph' in actor_temp:\n",
    "                        actor_max = 'Youssef Chahed'\n",
    "                        similarity_max, nb_identical_names = 1, 1 \n",
    "                    \n",
    "                    if similarity_max > 0.7 and nb_identical_names > 0: # This actor is already present in the list\n",
    "                        actor_temp = actor_max\n",
    "                    else:\n",
    "                        actor_tot_list.append(actor_temp)\n",
    "                        G.add_node(actor_temp)\n",
    "                        \n",
    "                    if actor_temp not in actor_temp_list:\n",
    "                        actor_temp_list.append(actor_temp)\n",
    "                    \n",
    "        if actor_temp_list != actor_buffer_list:\n",
    "            actor_buffer_list = actor_temp_list\n",
    "            \n",
    "            nb_actors = len(actor_temp_list)\n",
    "            #print(\"Actor list: \", nb_actors, actor_temp_list)\n",
    "            \n",
    "            # Edge creation between the actors of the article\n",
    "            for index1 in range(0, len(actor_temp_list)):\n",
    "                actor1 = actor_temp_list[index1]\n",
    "                offset1 = int(offset_temp_list[index1])\n",
    "                for index2 in range(index1 + 1, len(actor_temp_list)):\n",
    "                    actor2 = actor_temp_list[index2]\n",
    "                    offset2 = int(offset_temp_list[index2])\n",
    "                    weight_edge = np.abs(offset2 - offset1) / (max_offset_diff * nb_actors)\n",
    "                    #print(\"Weight: \", weight_edge)\n",
    "\n",
    "                    if G.has_edge(actor1, actor2):\n",
    "                        G[actor1][actor2]['weight'] += weight_edge\n",
    "                    else:\n",
    "                        G.add_edge(actor1, actor2, weight = weight_edge)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_offset_difference(actor_list, theme_list):\n",
    "    \n",
    "    offset_max = 0\n",
    "    offset_min = np.inf\n",
    "    \n",
    "    # 1: Actor loop\n",
    "    for actor_temp in actor_list.split(';'):\n",
    "        offset_temp = float(actor_temp.split(',')[1])\n",
    "        if offset_temp < offset_min:\n",
    "            offset_min = offset_temp\n",
    "        if offset_temp > offset_max:\n",
    "            offset_max = offset_temp\n",
    "            \n",
    "    # 2: Theme loop\n",
    "    if not isinstance(theme_list, float):\n",
    "        for theme_temp in theme_list.split(';'):\n",
    "            if theme_temp:\n",
    "                offset_temp = float(theme_temp.split(',')[1])\n",
    "                if offset_temp < offset_min:\n",
    "                    offset_min = offset_temp\n",
    "                if offset_temp > offset_max:\n",
    "                    offset_max = offset_temp\n",
    "            \n",
    "    # Computation of the maximum difference\n",
    "    max_offset_diff = offset_max - offset_min\n",
    "    return max_offset_diff\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theme_network_creation(G_themes, list_actor, dataframe, themes_of_interest, tf_idf):\n",
    "    \n",
    "    '''\n",
    "    Creation of a graph between the actors and the themes. For each theme mentioned in the articles, \n",
    "    we draw an edge between this theme and the closest actor in terms of offset. This will give us a\n",
    "    bipartite graph, with the actors on one side and the themes on the other side. The goal is to see if \n",
    "    some actors are strongly linked to very specific themes, as detected by GDELT\n",
    "    '''\n",
    "    uncommon_theme = ['GOV_DIVISIONOFPOWER', 'HATE_SPEECH', 'INFO_HOAX', 'POLITICAL_PRISONER', 'MEDIA_CENSORSHIP']\n",
    "    for actor_list, theme_list, doc_id in zip(dataframe.V2ENHANCEDPERSONS.unique(), \n",
    "                                              dataframe.V2ENHANCEDTHEMES.unique(), dataframe.GKGRECORDID):\n",
    "        \n",
    "        actor_list_temp, offset_list_temp = [], []\n",
    "    \n",
    "        if not isinstance(actor_list, float):\n",
    "            for actor in actor_list.split(';'):\n",
    "                actor_list_temp.append(actor.split(',')[0])\n",
    "                offset_list_temp.append(int(actor.split(',')[1]))\n",
    "    \n",
    "        # First, we need to get the themes and their respective offset in two separate lists\n",
    "        \n",
    "        if not isinstance(theme_list, float):\n",
    "            \n",
    "            number_theme = len(theme_list)\n",
    "            max_offset_diff = maximum_offset_difference(actor_list, theme_list)\n",
    "            \n",
    "            for theme in theme_list.split(';'):\n",
    "                if theme:\n",
    "                    theme_temp = theme.split(',')[0]\n",
    "                    offset_temp = int(theme.split(',')[1])\n",
    "                    \n",
    "                    if theme_temp in themes_of_interest:\n",
    "                    \n",
    "                        if not G_themes.has_node(theme_temp):\n",
    "                            G_themes.add_node(theme_temp)\n",
    "\n",
    "                        '''\n",
    "                        index_actor = np.argmin(np.abs([offset - offset_temp for offset in offset_list_temp]))\n",
    "                        actor_offset = actor_list_temp[index_actor]\n",
    "\n",
    "                        # We need to find this actor in the nodes of the network\n",
    "\n",
    "                        similarity_max = np.max([jellyfish.jaro_winkler(actor_offset, actor2) for \n",
    "                                                     actor2 in list_actor])\n",
    "                        index_max = np.argmax([jellyfish.jaro_winkler(actor_offset, actor2) for \n",
    "                                                      actor2 in list_actor])\n",
    "                        actor_max = list_actor[index_max]\n",
    "                        '''\n",
    "                        \n",
    "                        for (actor, offset_actor) in zip(actor_list_temp, offset_list_temp):\n",
    "                            offset_diff = np.abs(offset_actor - offset_temp)\n",
    "                            \n",
    "                            similarity_max = np.max([jellyfish.jaro_winkler(actor, actor2) for \n",
    "                                                     actor2 in list_actor])\n",
    "                            index_max = np.argmax(similarity_max)\n",
    "                            actor_max = list_actor[index_max]\n",
    "                            \n",
    "                            print(\"New actor: \", actor, actor_max)\n",
    "                        \n",
    "                        # The weight associated with this theme and article is extracted from the tf-idf dictionary\n",
    "                            weight_theme = tf_idf[doc_id][theme_temp] * max_offset_diff / offset_diff\n",
    "                        \n",
    "                            if theme_temp in uncommon_theme:\n",
    "                                print(\"Associated weight: \", theme_temp, weight_theme)\n",
    "\n",
    "                        # Now that we have the theme and the actor, we can draw an edge between the two\n",
    "\n",
    "                            if G_themes.has_edge(actor_max, theme_temp):\n",
    "                                G_themes[actor_max][theme_temp]['weight'] += weight_theme\n",
    "                            else:\n",
    "                                print(\"New edge! \", actor_max, theme_temp)\n",
    "                                G_themes.add_edge(actor_max, theme_temp, weight = weight_theme)\n",
    "                                      \n",
    "        \n",
    "    return G_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcam_extraction(dataframe, threshold):\n",
    "    \n",
    "    '''\n",
    "    Method used to read the different dictionary dimensions present in each article and to increment their count if their\n",
    "    density score (i.e number of reference words in the article divided by the word count) is greater than a pre-selected\n",
    "    threshold\n",
    "    '''\n",
    "    \n",
    "    gcam_dict = {}\n",
    "    print(np.max([2, 3]))\n",
    "    word_count = 0 # will be defined when reading the first entry of the GCAM feature\n",
    "    \n",
    "    for gcam_list in dataframe.V2GCAM.unique():\n",
    "        for gcam in gcam_list.split(','):\n",
    "            [dim_name, dim_count] = gcam.split(':')\n",
    "            \n",
    "            if dim_name == 'wc':\n",
    "                word_count = int(dim_count)\n",
    "            elif dim_name[0] == 'c': # we need to compute the density score for this dimension and compare it to the threshold\n",
    "                \n",
    "                density_score = int(dim_count) / word_count\n",
    "                if density_score > threshold:\n",
    "                    \n",
    "                    # Check if the dimension has already been added to the dictionary\n",
    "                    if dim_name in gcam_dict:\n",
    "                        gcam_dict[dim_name] += 1\n",
    "                    else:\n",
    "                        gcam_dict[dim_name] = 1\n",
    "                        \n",
    "    return gcam_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_dict_extraction(sorted_dict, k):\n",
    "\n",
    "    topk_dict = {}\n",
    "\n",
    "    for key, value in sorted_dict.items():\n",
    "        if len(topk_dict) < k:\n",
    "            print(\"New dict:\", key, value)\n",
    "            topk_dict[key] = value\n",
    "    \n",
    "    return topk_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def media_coverage_visualization(dataframe):\n",
    "    \n",
    "    '''\n",
    "    Goal: display a donut pie for each analyzed day, displaying the number of articles published by each media outlet \n",
    "    covering the news in Tunisia\n",
    "    '''\n",
    "    \n",
    "    dataframe.rename(columns={'V2.1DATE':'V2DATE'}, inplace=True)\n",
    "    DAY_BEGINNING = 24\n",
    "    \n",
    "    dict_ocurrences = [{}, {}, {}, {}, {}] # Five dictionaries for now, because five days of news are present in the dataframe\n",
    "    \n",
    "    for date, media in zip(dataframe.V2DATE, dataframe.V2SOURCECOMMONNAME):\n",
    "        day = int(str(date)[6:8])\n",
    "        month = int(str(date)[4:6])\n",
    "        \n",
    "        index_dict = day - DAY_BEGINNING\n",
    "        \n",
    "        if media in dict_ocurrences[index_dict]:\n",
    "            dict_ocurrences[index_dict][media] += 1\n",
    "        else:\n",
    "            dict_ocurrences[index_dict][media] = 1\n",
    "            \n",
    "    return dict_ocurrences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gdelt_repo(filename, date_beginning, date_ending):\n",
    "    \n",
    "    start_reading = False\n",
    "    \n",
    "    http = urllib3.PoolManager()\n",
    "            \n",
    "    with open(filename) as fileobject:\n",
    "        for line in fileobject:\n",
    "            \n",
    "            if line != '\\n':\n",
    "                line = line.strip()\n",
    "                time_line = line.split('v2/')[1][0:14]\n",
    "                \n",
    "                if time_line:\n",
    "                    time_line = int(line.split('v2/')[1][0:14])\n",
    "                \n",
    "                    if time_line == date_beginning:\n",
    "                        print(\"Start\")\n",
    "                        start_reading = True\n",
    "                    elif time_line == date_ending:\n",
    "                        print(\"End\")\n",
    "                        start_reading = False\n",
    "\n",
    "                    # While we are in the time interval, look for the GKG files\n",
    "                    if start_reading == True:\n",
    "                        #print(\"Download: \", line)\n",
    "                        gkg_string = line.split('00.')[1][0:3]\n",
    "\n",
    "                        if gkg_string == 'gkg':\n",
    "\n",
    "                            url = line.split(' ')[2]\n",
    "                            folder_dest = '/Users/aminmekacher/Documents/EPFL Master/MA2/GDELT Project/GDELT-Project/GKG Notebooks'\n",
    "                            file_dest = os.path.join(folder_dest, line.split('v2/')[1])\n",
    "                            with http.request('GET', url, preload_content=False) as r, open(file_dest, 'wb') as out_file:       \n",
    "                                shutil.copyfileobj(r, out_file)\n",
    "                            #urllib.request.urlretrieve(url, file_dest) \n",
    "                            #request.urlretrieve(url, file_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theme_list_extraction(dataframe):\n",
    "    '''\n",
    "    Method used to create a list with all the themes identified by GDELT in the dataframe\n",
    "    '''\n",
    "    \n",
    "    theme_list = []\n",
    "    \n",
    "    for theme_article in dataframe.V2ENHANCEDTHEMES.unique():\n",
    "        if not isinstance(theme_article, float):\n",
    "            for theme in theme_article.split(';'):\n",
    "                theme_temp = theme.split(',')[0]\n",
    "                if theme_temp not in theme_list and theme_temp != '':\n",
    "                    theme_list.append(theme_temp)\n",
    "    \n",
    "    return theme_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_computation(dataframe, themes_of_interest):\n",
    "    '''\n",
    "    Method used to compute, for each theme of interest, its tf-idf score with the articles present in the dataframe\n",
    "    '''\n",
    "    \n",
    "    tf_score, idf_score, tf_idf = {}, {}, {}\n",
    "    number_docs = len(dataframe.GKGRECORDID)\n",
    "    print(\"Num: \", number_docs)\n",
    "    \n",
    "    for theme_interest in themes_of_interest:\n",
    "        #print(\"New theme: \", theme_interest)\n",
    "        for theme_list, doc_id in zip(dataframe.V2ENHANCEDTHEMES.unique(), dataframe.GKGRECORDID):\n",
    "            theme_found = False\n",
    "            if not isinstance(theme_list, float):\n",
    "                for theme in theme_list.split(';'):\n",
    "                    if theme:\n",
    "                        theme_temp = theme.split(',')[0]\n",
    "                        # Checking if the theme is the one we are currently looking for\n",
    "                        if theme_temp == theme_interest:\n",
    "                            \n",
    "                            # We increment the tf_score of the theme in the current document\n",
    "                            if doc_id not in tf_score:\n",
    "                                tf_score[doc_id] = {}\n",
    "                            \n",
    "                            if theme_interest not in tf_score[doc_id]:\n",
    "                                tf_score[doc_id][theme_interest] = 0\n",
    "                                \n",
    "                            tf_score[doc_id][theme_interest] += 1\n",
    "                            \n",
    "                            #print(\"TF: \", tf_score)\n",
    "                            \n",
    "                            # We increment the idf score of the theme in the current document, if it has not been found yet\n",
    "                            if theme_found == False:\n",
    "                                    \n",
    "                                if theme_interest not in idf_score:\n",
    "                                    idf_score[theme_interest] = 0\n",
    "                                \n",
    "                                idf_score[theme_interest] += 1\n",
    "                                #print(\"NEW: \", idf_score[theme_interest])\n",
    "                                \n",
    "                                theme_found = True\n",
    "                                #print(\"IDF: \", idf_score)\n",
    "                                \n",
    "    # Last step: compute the tf-idf score for each theme for each document\n",
    "    for doc_id in dataframe.GKGRECORDID:\n",
    "        if doc_id in tf_score:\n",
    "            tf_temp = tf_score[doc_id]\n",
    "            max_tf = np.max(list(tf_temp.values()))\n",
    "            \n",
    "            for theme_interest in tf_temp:\n",
    "                tf_theme = tf_temp[theme_interest] / max_tf\n",
    "                idf_theme = np.log10(number_docs / idf_score[theme_interest])\n",
    "                if doc_id not in tf_idf:\n",
    "                    tf_idf[doc_id] = {}\n",
    "                if theme_interest not in tf_idf[doc_id]:\n",
    "                    tf_idf[doc_id][theme_interest] = 0\n",
    "                tf_idf[doc_id][theme_interest] = tf_theme * idf_theme\n",
    "                #print(\"New value: \", tf_idf)\n",
    "                                \n",
    "    return (tf_score, idf_score, tf_idf)\n",
    "                            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_similarity(dataframe):\n",
    "    '''\n",
    "    Method used to define the word2vec algorithm for the dataset, in order to compute the similarity between words\n",
    "    '''\n",
    "    \n",
    "    dataframe.rename(columns={'V2.1TRANSLATIONINFO':'TRANSLATIONINFO'}, inplace=True)\n",
    "    tokenized_article_list = [] # List used to append each article after they have been translated and tokenized\n",
    "    failures = 0\n",
    "    \n",
    "    for article_url, translation_info in zip(dataframe.V2DOCUMENTIDENTIFIER, dataframe.TRANSLATIONINFO):\n",
    "        #print(\"Translation: \", translation_info, article_url)\n",
    "        translator = Translator()\n",
    "        try:\n",
    "            #print(\"Works!!\")\n",
    "            scrapped_data = urllib.request.urlopen(article_url)\n",
    "\n",
    "            article = scrapped_data.read()\n",
    "            parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "            paragraphs = parsed_article.find_all('p')\n",
    "            article_text = \"\"\n",
    "\n",
    "            for p in paragraphs:\n",
    "                #print(\"Starting trans: \", p.text)\n",
    "                text_translated = translator.translate(text=p.text, dest='en')\n",
    "                #print(\"End trans\")\n",
    "                article_text += text_translated.text\n",
    "            #print(\"Article: \", article_text)\n",
    "\n",
    "            processed_article = article_text.lower()  \n",
    "            processed_article = re.sub('[^a-zA-Z]', ' ', processed_article)  \n",
    "            processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "            processed_article = re.sub(r'http\\S+', '', processed_article)\n",
    "\n",
    "            #print(\"Processed: \", processed_article)\n",
    "\n",
    "            # Preparing the dataset\n",
    "            all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "            all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "            # Removing Stop Words\n",
    "            from nltk.corpus import stopwords  \n",
    "            for i in range(len(all_words)):  \n",
    "                all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "\n",
    "            tokenized_article_list.append(all_words)\n",
    "            \n",
    "            print(\"New article tokenized: \", len(tokenized_article_list))\n",
    "\n",
    "            #print(\"Works! \", all_words)\n",
    "        except BaseException as e:\n",
    "            print('Failed to do something: ' + str(e))\n",
    "            failures += 1\n",
    "            print(\"Doesn't work \", failures)\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_presence(article_url, word):\n",
    "    \n",
    "    try:\n",
    "        scrapped_data = urllib.request.urlopen(article_url)\n",
    "    \n",
    "        article = scrapped_data.read()\n",
    "        parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "        paragraphs = parsed_article.find_all('p')\n",
    "        article_text = \"\"\n",
    "\n",
    "        for p in paragraphs:\n",
    "            article_text += p.text\n",
    "        #print(\"Article: \", article_text)\n",
    "    \n",
    "        return word in article_text\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        pass\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Start\n",
      "Start\n",
      "End\n",
      "End\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# First step: download all the CSV files for the time span of interest\n",
    "filename = 'masterfilelist.txt'\n",
    "\n",
    "#download_gdelt_repo(filename, date_beginning=20190330093000, date_ending=20190419000000)\n",
    "# UPDATE 01.05.2019: Start again from 20181005043000 (must be the first file to be downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_true = 'https://www.lacote.ch/dossiers/votations-federales-du-19-mai-2019/articles/loi-sur-les-armes-geneve-aeroport-craint-des-couts-lies-a-l-exclusion-de-schengen-en-cas-de-non-835444'\n",
    "url_false = 'https://www.lci.fr/replay/les-titres-du-jt-de-13h-2055-2119037.html'\n",
    "\n",
    "check_word_presence(url_false, 'Schengen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
